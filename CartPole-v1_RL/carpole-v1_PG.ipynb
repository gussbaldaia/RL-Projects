{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Policy Gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, states, actions):\n",
    "        # To see Cartpole learning, change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # Define size of state and actions\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # Hyper parameters for the DQN\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        self.gamma = 0.99\n",
    "        self.eps = 0.0001\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.states, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dense(self.actions, activation='softmax', kernel_initializer='he_normal'))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                    loss='categorical_crossentropy')\n",
    "        #model.summary()\n",
    "        return model\n",
    "\n",
    "    def print_Initial_W(self):\n",
    "        print(\"Initial Model Weights:\")\n",
    "        for layer in self.model.layers:\n",
    "            print(layer.get_weights())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # Sample an action from the policy\n",
    "        probabilities = self.model.predict(state, verbose=0)[0]      \n",
    "        # Clip probabilities to prevent NaN\n",
    "        probabilities = np.clip(probabilities, 1e-10, 1.0 - 1e-10)\n",
    "\n",
    "        action = np.random.choice(self.actions, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "        sum_reward = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_add = rewards[t] + self.gamma * sum_reward \n",
    "            discounted_rewards[t] = running_add\n",
    "        if normalize:\n",
    "            discounted_rewards = discounted_rewards-np.mean(discounted_rewards)/(np.std(discounted_rewards) + self.eps)\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        actions_one_hot = tf.one_hot(actions, self.actions, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the log probability of the taken actions\n",
    "            action_probabilities = self.model(states, training=True)\n",
    "            chosen_action_probabilities = tf.reduce_sum(actions_one_hot * action_probabilities, axis=1)\n",
    "            log_probabilities = -tf.math.log(chosen_action_probabilities)\n",
    "            # Compute the loss (negative log-likelihood multiplied by discounted rewards)\n",
    "            loss = tf.reduce_sum(log_probabilities * discounted_rewards)\n",
    "\n",
    "        # Update the model using the gradient of the loss with respect to the model parameters\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 |  Score: 14.0\n",
      "Episode: 1 |  Score: 30.0\n",
      "Episode: 2 |  Score: 11.0\n",
      "Episode: 3 |  Score: 12.0\n",
      "Episode: 4 |  Score: 19.0\n",
      "Episode: 5 |  Score: 16.0\n",
      "Episode: 6 |  Score: 16.0\n",
      "Episode: 7 |  Score: 11.0\n",
      "Episode: 8 |  Score: 10.0\n",
      "Episode: 9 |  Score: 14.0\n",
      "Episode: 10 |  Score: 14.0\n",
      "Episode: 11 |  Score: 9.0\n",
      "Episode: 12 |  Score: 8.0\n",
      "Episode: 13 |  Score: 33.0\n",
      "Episode: 14 |  Score: 10.0\n",
      "Episode: 15 |  Score: 16.0\n",
      "Episode: 16 |  Score: 15.0\n",
      "Episode: 17 |  Score: 13.0\n",
      "Episode: 18 |  Score: 10.0\n",
      "Episode: 19 |  Score: 12.0\n",
      "Episode: 20 |  Score: 13.0\n",
      "Episode: 21 |  Score: 13.0\n",
      "Episode: 22 |  Score: 12.0\n",
      "Episode: 23 |  Score: 16.0\n",
      "Episode: 24 |  Score: 21.0\n",
      "Episode: 25 |  Score: 12.0\n",
      "Episode: 26 |  Score: 10.0\n",
      "Episode: 27 |  Score: 11.0\n",
      "Episode: 28 |  Score: 9.0\n",
      "Episode: 29 |  Score: 13.0\n",
      "Episode: 30 |  Score: 10.0\n",
      "Episode: 31 |  Score: 35.0\n",
      "Episode: 32 |  Score: 15.0\n",
      "Episode: 33 |  Score: 28.0\n",
      "Episode: 34 |  Score: 21.0\n",
      "Episode: 35 |  Score: 10.0\n",
      "Episode: 36 |  Score: 12.0\n",
      "Episode: 37 |  Score: 16.0\n",
      "Episode: 38 |  Score: 10.0\n",
      "Episode: 39 |  Score: 20.0\n",
      "Episode: 40 |  Score: 20.0\n",
      "Episode: 41 |  Score: 127.0\n",
      "Episode: 42 |  Score: 11.0\n",
      "Episode: 43 |  Score: 10.0\n",
      "Episode: 44 |  Score: 11.0\n",
      "Episode: 45 |  Score: 9.0\n",
      "Episode: 46 |  Score: 19.0\n",
      "Episode: 47 |  Score: 14.0\n",
      "Episode: 48 |  Score: 14.0\n",
      "Episode: 49 |  Score: 12.0\n",
      "Episode: 50 |  Score: 9.0\n",
      "Episode: 51 |  Score: 11.0\n",
      "Episode: 52 |  Score: 12.0\n",
      "Episode: 53 |  Score: 8.0\n",
      "Episode: 54 |  Score: 16.0\n",
      "Episode: 55 |  Score: 11.0\n",
      "Episode: 56 |  Score: 10.0\n",
      "Episode: 57 |  Score: 8.0\n",
      "Episode: 58 |  Score: 9.0\n",
      "Episode: 59 |  Score: 8.0\n",
      "Episode: 60 |  Score: 11.0\n",
      "Episode: 61 |  Score: 11.0\n",
      "Episode: 62 |  Score: 11.0\n",
      "Episode: 63 |  Score: 10.0\n",
      "Episode: 64 |  Score: 9.0\n",
      "Episode: 65 |  Score: 9.0\n",
      "Episode: 66 |  Score: 12.0\n",
      "Episode: 67 |  Score: 9.0\n",
      "Episode: 68 |  Score: 10.0\n",
      "Episode: 69 |  Score: 14.0\n",
      "Episode: 70 |  Score: 11.0\n",
      "Episode: 71 |  Score: 10.0\n",
      "Episode: 72 |  Score: 11.0\n",
      "Episode: 73 |  Score: 13.0\n",
      "Episode: 74 |  Score: 21.0\n",
      "Episode: 75 |  Score: 10.0\n",
      "Episode: 76 |  Score: 11.0\n",
      "Episode: 77 |  Score: 13.0\n",
      "Episode: 78 |  Score: 12.0\n",
      "Episode: 79 |  Score: 11.0\n",
      "Episode: 80 |  Score: 10.0\n",
      "Episode: 81 |  Score: 15.0\n",
      "Episode: 82 |  Score: 11.0\n",
      "Episode: 83 |  Score: 15.0\n",
      "Episode: 84 |  Score: 9.0\n",
      "Episode: 85 |  Score: 10.0\n",
      "Episode: 86 |  Score: 12.0\n",
      "Episode: 87 |  Score: 17.0\n",
      "Episode: 88 |  Score: 9.0\n",
      "Episode: 89 |  Score: 10.0\n",
      "Episode: 90 |  Score: 10.0\n",
      "Episode: 91 |  Score: 10.0\n",
      "Episode: 92 |  Score: 15.0\n",
      "Episode: 93 |  Score: 12.0\n",
      "Episode: 94 |  Score: 14.0\n",
      "Episode: 95 |  Score: 16.0\n",
      "Episode: 96 |  Score: 10.0\n",
      "Episode: 97 |  Score: 16.0\n",
      "Episode: 98 |  Score: 26.0\n",
      "Episode: 99 |  Score: 19.0\n",
      "Episode: 100 |  Score: 15.0\n",
      "Episode: 101 |  Score: 11.0\n",
      "Episode: 102 |  Score: 15.0\n",
      "Episode: 103 |  Score: 19.0\n",
      "Episode: 104 |  Score: 19.0\n",
      "Episode: 105 |  Score: 15.0\n",
      "Episode: 106 |  Score: 14.0\n",
      "Episode: 107 |  Score: 10.0\n",
      "Episode: 108 |  Score: 13.0\n",
      "Episode: 109 |  Score: 13.0\n",
      "Episode: 110 |  Score: 12.0\n",
      "Episode: 111 |  Score: 32.0\n",
      "Episode: 112 |  Score: 24.0\n",
      "Episode: 113 |  Score: 11.0\n",
      "Episode: 114 |  Score: 26.0\n",
      "Episode: 115 |  Score: 12.0\n",
      "Episode: 116 |  Score: 10.0\n",
      "Episode: 117 |  Score: 27.0\n",
      "Episode: 118 |  Score: 23.0\n",
      "Episode: 119 |  Score: 21.0\n",
      "Episode: 120 |  Score: 10.0\n",
      "Episode: 121 |  Score: 10.0\n",
      "Episode: 122 |  Score: 14.0\n",
      "Episode: 123 |  Score: 21.0\n",
      "Episode: 124 |  Score: 13.0\n",
      "Episode: 125 |  Score: 8.0\n",
      "Episode: 126 |  Score: 21.0\n",
      "Episode: 127 |  Score: 26.0\n",
      "Episode: 128 |  Score: 12.0\n",
      "Episode: 129 |  Score: 9.0\n",
      "Episode: 130 |  Score: 13.0\n",
      "Episode: 131 |  Score: 8.0\n",
      "Episode: 132 |  Score: 9.0\n",
      "Episode: 133 |  Score: 10.0\n",
      "Episode: 134 |  Score: 13.0\n",
      "Episode: 135 |  Score: 10.0\n",
      "Episode: 136 |  Score: 14.0\n",
      "Episode: 137 |  Score: 17.0\n",
      "Episode: 138 |  Score: 16.0\n",
      "Episode: 139 |  Score: 9.0\n",
      "Episode: 140 |  Score: 20.0\n",
      "Episode: 141 |  Score: 14.0\n",
      "Episode: 142 |  Score: 10.0\n",
      "Episode: 143 |  Score: 25.0\n",
      "Episode: 144 |  Score: 10.0\n",
      "Episode: 145 |  Score: 17.0\n",
      "Episode: 146 |  Score: 10.0\n",
      "Episode: 147 |  Score: 11.0\n",
      "Episode: 148 |  Score: 11.0\n",
      "Episode: 149 |  Score: 10.0\n",
      "Episode: 150 |  Score: 20.0\n",
      "Episode: 151 |  Score: 15.0\n",
      "Episode: 152 |  Score: 10.0\n",
      "Episode: 153 |  Score: 11.0\n",
      "Episode: 154 |  Score: 12.0\n",
      "Episode: 155 |  Score: 10.0\n",
      "Episode: 156 |  Score: 18.0\n",
      "Episode: 157 |  Score: 20.0\n",
      "Episode: 158 |  Score: 11.0\n",
      "Episode: 159 |  Score: 8.0\n",
      "Episode: 160 |  Score: 20.0\n",
      "Episode: 161 |  Score: 19.0\n",
      "Episode: 162 |  Score: 10.0\n",
      "Episode: 163 |  Score: 11.0\n",
      "Episode: 164 |  Score: 11.0\n",
      "Episode: 165 |  Score: 9.0\n",
      "Episode: 166 |  Score: 13.0\n",
      "Episode: 167 |  Score: 22.0\n",
      "Episode: 168 |  Score: 8.0\n",
      "Episode: 169 |  Score: 21.0\n",
      "Episode: 170 |  Score: 9.0\n",
      "Episode: 171 |  Score: 17.0\n",
      "Episode: 172 |  Score: 10.0\n",
      "Episode: 173 |  Score: 17.0\n",
      "Episode: 174 |  Score: 9.0\n",
      "Episode: 175 |  Score: 12.0\n",
      "Episode: 176 |  Score: 9.0\n",
      "Episode: 177 |  Score: 9.0\n",
      "Episode: 178 |  Score: 9.0\n",
      "Episode: 179 |  Score: 10.0\n",
      "Episode: 180 |  Score: 11.0\n",
      "Episode: 181 |  Score: 14.0\n",
      "Episode: 182 |  Score: 11.0\n",
      "Episode: 183 |  Score: 13.0\n",
      "Episode: 184 |  Score: 12.0\n",
      "Episode: 185 |  Score: 12.0\n",
      "Episode: 186 |  Score: 11.0\n",
      "Episode: 187 |  Score: 8.0\n",
      "Episode: 188 |  Score: 11.0\n",
      "Episode: 189 |  Score: 10.0\n",
      "Episode: 190 |  Score: 13.0\n",
      "Episode: 191 |  Score: 10.0\n",
      "Episode: 192 |  Score: 9.0\n",
      "Episode: 193 |  Score: 9.0\n",
      "Episode: 194 |  Score: 11.0\n",
      "Episode: 195 |  Score: 9.0\n",
      "Episode: 196 |  Score: 10.0\n",
      "Episode: 197 |  Score: 10.0\n",
      "Episode: 198 |  Score: 10.0\n",
      "Episode: 199 |  Score: 9.0\n",
      "Episode: 200 |  Score: 10.0\n",
      "Episode: 201 |  Score: 8.0\n",
      "Episode: 202 |  Score: 10.0\n",
      "Episode: 203 |  Score: 8.0\n",
      "Episode: 204 |  Score: 8.0\n",
      "Episode: 205 |  Score: 9.0\n",
      "Episode: 206 |  Score: 10.0\n",
      "Episode: 207 |  Score: 10.0\n",
      "Episode: 208 |  Score: 8.0\n",
      "Episode: 209 |  Score: 11.0\n",
      "Episode: 210 |  Score: 10.0\n",
      "Episode: 211 |  Score: 11.0\n",
      "Episode: 212 |  Score: 10.0\n",
      "Episode: 213 |  Score: 10.0\n",
      "Episode: 214 |  Score: 9.0\n",
      "Episode: 215 |  Score: 9.0\n",
      "Episode: 216 |  Score: 9.0\n",
      "Episode: 217 |  Score: 9.0\n",
      "Episode: 218 |  Score: 9.0\n",
      "Episode: 219 |  Score: 10.0\n",
      "Episode: 220 |  Score: 8.0\n",
      "Episode: 221 |  Score: 9.0\n",
      "Episode: 222 |  Score: 10.0\n",
      "Episode: 223 |  Score: 10.0\n",
      "Episode: 224 |  Score: 10.0\n",
      "Episode: 225 |  Score: 9.0\n",
      "Episode: 226 |  Score: 9.0\n",
      "Episode: 227 |  Score: 9.0\n",
      "Episode: 228 |  Score: 8.0\n",
      "Episode: 229 |  Score: 10.0\n",
      "Episode: 230 |  Score: 10.0\n",
      "Episode: 231 |  Score: 10.0\n",
      "Episode: 232 |  Score: 10.0\n",
      "Episode: 233 |  Score: 9.0\n",
      "Episode: 234 |  Score: 10.0\n",
      "Episode: 235 |  Score: 9.0\n",
      "Episode: 236 |  Score: 8.0\n",
      "Episode: 237 |  Score: 9.0\n",
      "Episode: 238 |  Score: 10.0\n",
      "Episode: 239 |  Score: 10.0\n",
      "Episode: 240 |  Score: 9.0\n",
      "Episode: 241 |  Score: 9.0\n",
      "Episode: 242 |  Score: 9.0\n",
      "Episode: 243 |  Score: 10.0\n",
      "Episode: 244 |  Score: 11.0\n",
      "Episode: 245 |  Score: 10.0\n",
      "Episode: 246 |  Score: 10.0\n",
      "Episode: 247 |  Score: 8.0\n",
      "Episode: 248 |  Score: 8.0\n",
      "Episode: 249 |  Score: 8.0\n",
      "Episode: 250 |  Score: 8.0\n",
      "Episode: 251 |  Score: 8.0\n",
      "Episode: 252 |  Score: 9.0\n",
      "Episode: 253 |  Score: 10.0\n",
      "Episode: 254 |  Score: 8.0\n",
      "Episode: 255 |  Score: 8.0\n",
      "Episode: 256 |  Score: 8.0\n",
      "Episode: 257 |  Score: 10.0\n",
      "Episode: 258 |  Score: 9.0\n",
      "Episode: 259 |  Score: 10.0\n",
      "Episode: 260 |  Score: 9.0\n",
      "Episode: 261 |  Score: 9.0\n",
      "Episode: 262 |  Score: 9.0\n",
      "Episode: 263 |  Score: 9.0\n",
      "Episode: 264 |  Score: 9.0\n",
      "Episode: 265 |  Score: 9.0\n",
      "Episode: 266 |  Score: 9.0\n",
      "Episode: 267 |  Score: 10.0\n",
      "Episode: 268 |  Score: 9.0\n",
      "Episode: 269 |  Score: 8.0\n",
      "Episode: 270 |  Score: 10.0\n",
      "Episode: 271 |  Score: 9.0\n",
      "Episode: 272 |  Score: 9.0\n",
      "Episode: 273 |  Score: 11.0\n",
      "Episode: 274 |  Score: 10.0\n",
      "Episode: 275 |  Score: 8.0\n",
      "Episode: 276 |  Score: 8.0\n",
      "Episode: 277 |  Score: 10.0\n",
      "Episode: 278 |  Score: 10.0\n",
      "Episode: 279 |  Score: 10.0\n",
      "Episode: 280 |  Score: 10.0\n",
      "Episode: 281 |  Score: 10.0\n",
      "Episode: 282 |  Score: 9.0\n",
      "Episode: 283 |  Score: 9.0\n",
      "Episode: 284 |  Score: 10.0\n",
      "Episode: 285 |  Score: 9.0\n",
      "Episode: 286 |  Score: 8.0\n",
      "Episode: 287 |  Score: 9.0\n",
      "Episode: 288 |  Score: 10.0\n",
      "Episode: 289 |  Score: 10.0\n",
      "Episode: 290 |  Score: 10.0\n",
      "Episode: 291 |  Score: 8.0\n",
      "Episode: 292 |  Score: 11.0\n",
      "Episode: 293 |  Score: 10.0\n",
      "Episode: 294 |  Score: 9.0\n",
      "Episode: 295 |  Score: 9.0\n",
      "Episode: 296 |  Score: 10.0\n",
      "Episode: 297 |  Score: 9.0\n",
      "Episode: 298 |  Score: 10.0\n",
      "Episode: 299 |  Score: 9.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #env = gym.make('CartPole-v1', render_mode='human')\n",
    "    env = gym.make('CartPole-v1')\n",
    "    states = env.observation_space.shape[0]\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    EPISODES = 300\n",
    "\n",
    "    agent = PolicyGradientAgent(states, actions)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, states])\n",
    "\n",
    "        states_batch, actions_batch, rewards_batch = [], [], []\n",
    "\n",
    "        #agent.print_Initial_W()\n",
    "\n",
    "        while not done:\n",
    "            #if agent.render:\n",
    "            #    env.render()\n",
    "\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, states])\n",
    "\n",
    "            states_batch.append(state)\n",
    "            actions_batch.append(action)\n",
    "            rewards_batch.append(reward)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Train the model at the end of the episode\n",
    "        states_batch = np.vstack(states_batch)\n",
    "        actions_batch = np.array(actions_batch)\n",
    "        rewards_batch = np.array(rewards_batch)\n",
    "        agent.train(states_batch, actions_batch, rewards_batch)\n",
    "\n",
    "        scores.append(score)\n",
    "        episodes.append(e)\n",
    "\n",
    "        print(\"Episode:\", e, \"|  Score:\", score)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
