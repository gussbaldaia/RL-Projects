{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Policy Gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, states, actions):\n",
    "        # To see Cartpole learning, change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # Define size of state and actions\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # Hyper parameters for the DQN\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.states, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.actions, activation='softmax'))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                    loss='categorical_crossentropy')\n",
    "        #model.summary()\n",
    "        return model\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "       # Sample an action from the policy\n",
    "        probabilities = self.model.predict(state, verbose=0)[0]\n",
    "        action = np.random.choice(self.actions, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "        sum_reward = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_add = rewards[t] + self.gamma * sum_reward \n",
    "            discounted_rewards[t] = running_add\n",
    "        # Normalize the discounted rewards\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        actions_one_hot = tf.one_hot(actions, self.actions, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the log probability of the taken actions\n",
    "            action_probabilities = self.model(states, training=True)\n",
    "            chosen_action_probabilities = tf.reduce_sum(actions_one_hot * action_probabilities, axis=1)\n",
    "            log_probabilities = tf.math.log(chosen_action_probabilities)\n",
    "\n",
    "            # Compute the loss (negative log-likelihood multiplied by discounted rewards)\n",
    "            loss = -tf.reduce_sum(log_probabilities * discounted_rewards)\n",
    "\n",
    "        # Update the model using the gradient of the loss with respect to the model parameters\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\Gustavo\\AppData\\Local\\Temp\\ipykernel_18284\\3811001964.py:43: RuntimeWarning: invalid value encountered in divide\n",
      "  discounted_rewards /= np.std(discounted_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 22.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrender \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     22\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 24\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     26\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, [\u001b[38;5;241m1\u001b[39m, states])\n",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m, in \u001b[0;36mPolicyGradientAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     30\u001b[0m    \u001b[38;5;66;03m# Sample an action from the policy\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions, p\u001b[38;5;241m=\u001b[39mprobabilities)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:971\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    states = env.observation_space.shape[0]\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    EPISODES = 300\n",
    "\n",
    "    agent = PolicyGradientAgent(states, actions)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, states])\n",
    "\n",
    "        states_batch, actions_batch, rewards_batch = [], [], []\n",
    "\n",
    "        while not done:\n",
    "            if agent.render == 1:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, states])\n",
    "\n",
    "            states_batch.append(state)\n",
    "            actions_batch.append(action)\n",
    "            rewards_batch.append(reward)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Train the model at the end of the episode\n",
    "        states_batch = np.vstack(states_batch)\n",
    "        actions_batch = np.array(actions_batch)\n",
    "        rewards_batch = np.array(rewards_batch)\n",
    "        agent.train(states_batch, actions_batch, rewards_batch)\n",
    "\n",
    "        scores.append(score)\n",
    "        episodes.append(e)\n",
    "\n",
    "        print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "        if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
