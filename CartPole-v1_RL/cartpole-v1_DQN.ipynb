{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Agent***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 25.0\n",
      "Episode: 2, Score: 17.0\n",
      "Episode: 3, Score: 32.0\n",
      "Episode: 4, Score: 27.0\n",
      "Episode: 5, Score: 35.0\n",
      "Episode: 6, Score: 28.0\n",
      "Episode: 7, Score: 19.0\n",
      "Episode: 8, Score: 17.0\n",
      "Episode: 9, Score: 24.0\n",
      "Episode: 10, Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for episode in range (1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Deep Q Networks (DQN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, states, actions):\n",
    "        # To see Cartpole learning, change to True\n",
    "        self.render = 1\n",
    "        self.load_model = False\n",
    "\n",
    "        # Define size of state and actions\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # Hyper parameters for the DQN\n",
    "        self.discount_factor = 0.975\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.train_start = 1000\n",
    "        self.batch_size = 128\n",
    "\n",
    "        self.buffer_size = 5000\n",
    "        self.memory = deque(maxlen=self.buffer_size)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.states, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.actions, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                    loss='mse')\n",
    "        #model.summary()\n",
    "        return model\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # Store the experience in the replay buffer\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Decay epsilon for exploration-exploitation trade-off\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # Function to perform epsilon-greedy action selection\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        q_values = self.model.predict(state, verbose = 0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Update target network with weights from main network\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def training_loop(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.states))\n",
    "        update_target = np.zeros((batch_size, self.states))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            update_target[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])       \n",
    "\n",
    "        target = self.model.predict(update_input, verbose=0)\n",
    "        target_val = self.target_model.predict(update_target, verbose=0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 24.0   memory length: 25   epsilon: 0.9975029977012647\n",
      "episode: 1   score: 19.0   memory length: 45   epsilon: 0.9955098858248879\n",
      "episode: 2   score: 39.0   memory length: 85   epsilon: 0.9915356014321521\n",
      "episode: 3   score: 20.0   memory length: 106   epsilon: 0.9894554575757586\n",
      "episode: 4   score: 20.0   memory length: 127   epsilon: 0.9873796776559268\n",
      "episode: 5   score: 20.0   memory length: 148   epsilon: 0.9853082525175486\n",
      "episode: 6   score: 17.0   memory length: 166   epsilon: 0.9835362043809335\n",
      "episode: 7   score: 9.0   memory length: 176   epsilon: 0.982553110649841\n",
      "episode: 8   score: 18.0   memory length: 195   epsilon: 0.9806879389537123\n",
      "episode: 9   score: 38.0   memory length: 234   epsilon: 0.9768705139349746\n",
      "episode: 10   score: 9.0   memory length: 244   epsilon: 0.975894082895567\n",
      "episode: 11   score: 57.0   memory length: 302   epsilon: 0.9702499986731349\n",
      "episode: 12   score: 21.0   memory length: 324   epsilon: 0.9681176884600758\n",
      "episode: 13   score: 16.0   memory length: 341   epsilon: 0.9664732043716606\n",
      "episode: 14   score: 43.0   memory length: 385   epsilon: 0.9622298523220771\n",
      "episode: 15   score: 15.0   memory length: 401   epsilon: 0.9606914386955115\n",
      "episode: 16   score: 34.0   memory length: 436   epsilon: 0.9573347284914394\n",
      "episode: 17   score: 18.0   memory length: 455   epsilon: 0.9555174286224053\n",
      "episode: 18   score: 11.0   memory length: 467   epsilon: 0.9543714381393948\n",
      "episode: 19   score: 27.0   memory length: 495   epsilon: 0.9517028025120734\n",
      "episode: 20   score: 12.0   memory length: 508   epsilon: 0.9504663309248748\n",
      "episode: 21   score: 21.0   memory length: 530   epsilon: 0.9483774991110414\n",
      "episode: 22   score: 32.0   memory length: 563   epsilon: 0.9452528556267011\n",
      "episode: 23   score: 24.0   memory length: 588   epsilon: 0.9428925570733151\n",
      "episode: 24   score: 26.0   memory length: 615   epsilon: 0.940350053965786\n",
      "episode: 25   score: 44.0   memory length: 660   epsilon: 0.9361277748589067\n",
      "episode: 26   score: 19.0   memory length: 680   epsilon: 0.9342572968852291\n",
      "episode: 27   score: 28.0   memory length: 709   epsilon: 0.9315517403973295\n",
      "episode: 28   score: 11.0   memory length: 721   epsilon: 0.9304344929281061\n",
      "episode: 29   score: 30.0   memory length: 752   epsilon: 0.9275544683410445\n",
      "episode: 30   score: 42.0   memory length: 795   epsilon: 0.9235743485085159\n",
      "episode: 31   score: 9.0   memory length: 805   epsilon: 0.9226511896576549\n",
      "episode: 32   score: 23.0   memory length: 829   epsilon: 0.9204393714532942\n",
      "episode: 33   score: 28.0   memory length: 858   epsilon: 0.9177738308988277\n",
      "episode: 34   score: 42.0   memory length: 901   epsilon: 0.9138356796087268\n",
      "episode: 35   score: 10.0   memory length: 912   epsilon: 0.9128309628200283\n",
      "episode: 36   score: 43.0   memory length: 956   epsilon: 0.9088231298873775\n",
      "episode: 37   score: 31.0   memory length: 988   epsilon: 0.905919399129966\n",
      "episode: 38   score: 12.0   memory length: 1001   epsilon: 0.9047424102692004\n",
      "episode: 39   score: 12.0   memory length: 1014   epsilon: 0.9035669505762389\n",
      "episode: 40   score: 7.0   memory length: 1022   epsilon: 0.9028443499639306\n",
      "episode: 41   score: 7.0   memory length: 1030   epsilon: 0.9021223272298245\n",
      "episode: 42   score: 15.0   memory length: 1046   epsilon: 0.9006800135480254\n",
      "episode: 43   score: 16.0   memory length: 1063   epsilon: 0.8991500818375644\n",
      "episode: 44   score: 16.0   memory length: 1080   epsilon: 0.897622748931344\n",
      "episode: 45   score: 18.0   memory length: 1099   epsilon: 0.8959187997738268\n",
      "episode: 46   score: 45.0   memory length: 1145   epsilon: 0.8918068324690056\n",
      "episode: 47   score: 8.0   memory length: 1154   epsilon: 0.8910045272953426\n",
      "episode: 48   score: 10.0   memory length: 1165   epsilon: 0.8900249122208217\n",
      "episode: 49   score: 40.0   memory length: 1206   epsilon: 0.8863830988063378\n",
      "episode: 50   score: 15.0   memory length: 1222   epsilon: 0.8849659490117532\n",
      "episode: 51   score: 24.0   memory length: 1247   epsilon: 0.8827561870027681\n",
      "episode: 52   score: 28.0   memory length: 1276   epsilon: 0.8801997748270841\n",
      "episode: 53   score: 19.0   memory length: 1296   epsilon: 0.878441046654001\n",
      "episode: 54   score: 11.0   memory length: 1308   epsilon: 0.8773874969758937\n",
      "episode: 55   score: 20.0   memory length: 1329   epsilon: 0.8755468245795875\n",
      "episode: 56   score: 39.0   memory length: 1369   epsilon: 0.8720514579040944\n",
      "episode: 57   score: 32.0   memory length: 1402   epsilon: 0.8691782877703627\n",
      "episode: 58   score: 24.0   memory length: 1427   epsilon: 0.8670079475877893\n",
      "episode: 59   score: 9.0   memory length: 1437   epsilon: 0.8661413296897552\n",
      "episode: 60   score: 24.0   memory length: 1462   epsilon: 0.8639785727984902\n",
      "episode: 61   score: 17.0   memory length: 1480   epsilon: 0.8624247325499271\n",
      "episode: 62   score: 25.0   memory length: 1506   epsilon: 0.860185228884663\n",
      "episode: 63   score: 30.0   memory length: 1537   epsilon: 0.8575226506726076\n",
      "episode: 64   score: 15.0   memory length: 1553   epsilon: 0.8561516429786556\n",
      "episode: 65   score: 23.0   memory length: 1577   epsilon: 0.8540992402820999\n",
      "episode: 66   score: 24.0   memory length: 1602   epsilon: 0.8519665525157675\n",
      "episode: 67   score: 20.0   memory length: 1623   epsilon: 0.8501792107526394\n",
      "episode: 68   score: 77.0   memory length: 1701   epsilon: 0.8435732792333273\n",
      "episode: 69   score: 30.0   memory length: 1732   epsilon: 0.8409621208942437\n",
      "episode: 70   score: 35.0   memory length: 1768   epsilon: 0.8379399493208673\n",
      "episode: 71   score: 19.0   memory length: 1788   epsilon: 0.8362656605532838\n",
      "episode: 72   score: 13.0   memory length: 1802   epsilon: 0.8350956493259434\n",
      "episode: 73   score: 17.0   memory length: 1820   epsilon: 0.8335937541723177\n",
      "episode: 74   score: 49.0   memory length: 1870   epsilon: 0.8294359806056876\n",
      "episode: 75   score: 43.0   memory length: 1914   epsilon: 0.8257942977816\n",
      "episode: 76   score: 14.0   memory length: 1929   epsilon: 0.8245564730433167\n",
      "episode: 77   score: 12.0   memory length: 1942   epsilon: 0.8234851925466452\n",
      "episode: 78   score: 18.0   memory length: 1961   epsilon: 0.8219219780428481\n",
      "episode: 79   score: 26.0   memory length: 1988   epsilon: 0.8197056712455956\n",
      "episode: 80   score: 62.0   memory length: 2051   epsilon: 0.8145575018659459\n",
      "episode: 81   score: 19.0   memory length: 2071   epsilon: 0.8129299335932668\n",
      "episode: 82   score: 23.0   memory length: 2095   epsilon: 0.8109811437947528\n",
      "episode: 83   score: 14.0   memory length: 2110   epsilon: 0.8097655232403761\n",
      "episode: 84   score: 23.0   memory length: 2134   epsilon: 0.8078243192993382\n",
      "episode: 85   score: 17.0   memory length: 2152   epsilon: 0.8063714708368706\n",
      "episode: 86   score: 14.0   memory length: 2167   epsilon: 0.8051627599538709\n",
      "episode: 87   score: 40.0   memory length: 2208   epsilon: 0.8018681863978047\n",
      "episode: 88   score: 27.0   memory length: 2236   epsilon: 0.7996259839123566\n",
      "episode: 89   score: 57.0   memory length: 2294   epsilon: 0.7950013463838091\n",
      "episode: 90   score: 90.0   memory length: 2385   epsilon: 0.7877992930682217\n",
      "episode: 91   score: 26.0   memory length: 2412   epsilon: 0.7856749978495254\n",
      "episode: 92   score: 17.0   memory length: 2430   epsilon: 0.7842619842952728\n",
      "episode: 93   score: 47.0   memory length: 2478   epsilon: 0.7805063596964901\n",
      "episode: 94   score: 16.0   memory length: 2495   epsilon: 0.7791805598430969\n",
      "episode: 95   score: 39.0   memory length: 2535   epsilon: 0.7760699075209033\n",
      "episode: 96   score: 107.0   memory length: 2643   epsilon: 0.7677330358146467\n",
      "episode: 97   score: 52.0   memory length: 2696   epsilon: 0.7636746121236078\n",
      "episode: 98   score: 32.0   memory length: 2729   epsilon: 0.7611585139420669\n",
      "episode: 99   score: 17.0   memory length: 2747   epsilon: 0.7597895925686249\n",
      "episode: 100   score: 36.0   memory length: 2784   epsilon: 0.7569834253762573\n",
      "episode: 101   score: 10.0   memory length: 2795   epsilon: 0.7561511598243502\n",
      "episode: 102   score: 34.0   memory length: 2830   epsilon: 0.7535091249193135\n",
      "episode: 103   score: 21.0   memory length: 2852   epsilon: 0.7518531442907166\n",
      "episode: 104   score: 16.0   memory length: 2869   epsilon: 0.7505760159546176\n",
      "episode: 105   score: 124.0   memory length: 2994   epsilon: 0.7412517476265477\n",
      "episode: 106   score: 19.0   memory length: 3014   epsilon: 0.7397706516649472\n",
      "episode: 107   score: 20.0   memory length: 3035   epsilon: 0.738218685831367\n",
      "episode: 108   score: 9.0   memory length: 3045   epsilon: 0.7374807992553735\n",
      "episode: 109   score: 29.0   memory length: 3075   epsilon: 0.7352715619069324\n",
      "episode: 110   score: 44.0   memory length: 3120   epsilon: 0.7319701086442577\n",
      "episode: 111   score: 22.0   memory length: 3143   epsilon: 0.7302884279830798\n",
      "episode: 112   score: 31.0   memory length: 3175   epsilon: 0.7279551236245312\n",
      "episode: 113   score: 19.0   memory length: 3195   epsilon: 0.726500595662501\n",
      "episode: 114   score: 22.0   memory length: 3218   epsilon: 0.7248314810529952\n",
      "episode: 115   score: 26.0   memory length: 3245   epsilon: 0.7228769780937903\n",
      "episode: 116   score: 12.0   memory length: 3258   epsilon: 0.7219378016596203\n",
      "episode: 117   score: 33.0   memory length: 3292   epsilon: 0.7194872588883153\n",
      "episode: 118   score: 17.0   memory length: 3310   epsilon: 0.7181932820509412\n",
      "episode: 119   score: 77.0   memory length: 3388   epsilon: 0.7126128872602253\n",
      "episode: 120   score: 26.0   memory length: 3415   epsilon: 0.7106913316527146\n",
      "episode: 121   score: 27.0   memory length: 3443   epsilon: 0.7087040800105503\n",
      "episode: 122   score: 17.0   memory length: 3461   epsilon: 0.7074294964056882\n",
      "episode: 123   score: 59.0   memory length: 3521   epsilon: 0.7031974167555616\n",
      "episode: 124   score: 37.0   memory length: 3559   epsilon: 0.7005302041227441\n",
      "episode: 125   score: 45.0   memory length: 3605   epsilon: 0.697315005048766\n",
      "episode: 126   score: 71.0   memory length: 3677   epsilon: 0.6923121188677195\n",
      "episode: 127   score: 27.0   memory length: 3705   epsilon: 0.6903762596081019\n",
      "episode: 128   score: 28.0   memory length: 3734   epsilon: 0.6883769688618566\n",
      "episode: 129   score: 35.0   memory length: 3770   epsilon: 0.685903143637899\n",
      "episode: 130   score: 52.0   memory length: 3823   epsilon: 0.6822772926740361\n",
      "episode: 131   score: 20.0   memory length: 3844   epsilon: 0.680845942234715\n",
      "episode: 132   score: 81.0   memory length: 3926   epsilon: 0.6752855562253139\n",
      "episode: 133   score: 24.0   memory length: 3951   epsilon: 0.6735993666391165\n",
      "episode: 134   score: 37.0   memory length: 3989   epsilon: 0.6710444187719203\n",
      "episode: 135   score: 26.0   memory length: 4016   epsilon: 0.6692349522455187\n",
      "episode: 136   score: 31.0   memory length: 4048   epsilon: 0.6670967164866962\n",
      "episode: 137   score: 48.0   memory length: 4097   epsilon: 0.6638357753568291\n",
      "episode: 138   score: 57.0   memory length: 4155   epsilon: 0.6599964806099436\n",
      "episode: 139   score: 32.0   memory length: 4188   epsilon: 0.6578219734071071\n",
      "episode: 140   score: 15.0   memory length: 4204   epsilon: 0.6567702472677635\n",
      "episode: 141   score: 13.0   memory length: 4218   epsilon: 0.6558513663435152\n",
      "episode: 142   score: 31.0   memory length: 4250   epsilon: 0.6537558917433272\n",
      "episode: 143   score: 73.0   memory length: 4324   epsilon: 0.6489357137871091\n",
      "episode: 144   score: 75.0   memory length: 4400   epsilon: 0.6440222514931265\n",
      "episode: 145   score: 18.0   memory length: 4419   epsilon: 0.6427997098695319\n",
      "episode: 146   score: 33.0   memory length: 4453   epsilon: 0.6406177931188141\n",
      "episode: 147   score: 28.0   memory length: 4482   epsilon: 0.6387626000877129\n",
      "episode: 148   score: 57.0   memory length: 4540   epsilon: 0.6350683160703967\n",
      "episode: 149   score: 40.0   memory length: 4581   epsilon: 0.6324697367712983\n",
      "episode: 150   score: 28.0   memory length: 4610   epsilon: 0.6306381400522497\n",
      "episode: 151   score: 105.0   memory length: 4716   epsilon: 0.6239883494301238\n",
      "episode: 152   score: 39.0   memory length: 4756   epsilon: 0.6214972569822229\n",
      "episode: 153   score: 71.0   memory length: 4828   epsilon: 0.6170383251995953\n",
      "episode: 154   score: 51.0   memory length: 4880   epsilon: 0.6138378942168917\n",
      "episode: 155   score: 99.0   memory length: 4980   epsilon: 0.607729801233138\n",
      "episode: 156   score: 20.0   memory length: 5000   epsilon: 0.6064548440752141\n",
      "episode: 157   score: 69.0   memory length: 5000   epsilon: 0.6022242729093658\n",
      "episode: 158   score: 55.0   memory length: 5000   epsilon: 0.5988610745633159\n",
      "episode: 159   score: 17.0   memory length: 5000   epsilon: 0.5977840403980587\n",
      "episode: 160   score: 59.0   memory length: 5000   epsilon: 0.5942078965061331\n",
      "episode: 161   score: 73.0   memory length: 5000   epsilon: 0.5898267691766154\n",
      "episode: 162   score: 93.0   memory length: 5000   epsilon: 0.5843080999912418\n",
      "episode: 163   score: 97.0   memory length: 5000   epsilon: 0.5786095641150684\n",
      "episode: 164   score: 19.0   memory length: 5000   epsilon: 0.5774534436856754\n",
      "episode: 165   score: 74.0   memory length: 5000   epsilon: 0.5731385282686384\n",
      "episode: 166   score: 11.0   memory length: 5000   epsilon: 0.5724511401800828\n",
      "episode: 167   score: 56.0   memory length: 5000   epsilon: 0.569197288273922\n",
      "episode: 168   score: 103.0   memory length: 5000   epsilon: 0.5633080192907329\n",
      "episode: 169   score: 69.0   memory length: 5000   epsilon: 0.5593784362604647\n",
      "episode: 170   score: 50.0   memory length: 5000   epsilon: 0.5565327266755089\n",
      "episode: 171   score: 58.0   memory length: 5000   epsilon: 0.5532586877960566\n",
      "episode: 172   score: 36.0   memory length: 5000   epsilon: 0.5512153110589036\n",
      "episode: 173   score: 23.0   memory length: 5000   epsilon: 0.5498939145515465\n",
      "episode: 174   score: 54.0   memory length: 5000   epsilon: 0.5468776495384128\n",
      "episode: 175   score: 32.0   memory length: 5000   epsilon: 0.5450758378273978\n",
      "episode: 176   score: 75.0   memory length: 5000   epsilon: 0.5409487578722881\n",
      "episode: 177   score: 59.0   memory length: 5000   epsilon: 0.537712621633152\n",
      "episode: 178   score: 93.0   memory length: 5000   epsilon: 0.5326815544950225\n",
      "episode: 179   score: 54.0   memory length: 5000   epsilon: 0.5297597023096327\n",
      "episode: 180   score: 32.0   memory length: 5000   epsilon: 0.5280142895350369\n",
      "episode: 181   score: 29.0   memory length: 5000   epsilon: 0.5264325413862998\n",
      "episode: 182   score: 59.0   memory length: 5000   epsilon: 0.523283246005085\n",
      "episode: 183   score: 163.0   memory length: 5000   epsilon: 0.514770966637593\n",
      "episode: 184   score: 45.0   memory length: 5000   epsilon: 0.5124083402647351\n",
      "episode: 185   score: 52.0   memory length: 5000   epsilon: 0.5096996250595732\n",
      "episode: 186   score: 11.0   memory length: 5000   epsilon: 0.5090883217991456\n",
      "episode: 187   score: 68.0   memory length: 5000   epsilon: 0.5055875289615416\n",
      "episode: 188   score: 124.0   memory length: 5000   epsilon: 0.49930670772138047\n",
      "episode: 189   score: 42.0   memory length: 5000   epsilon: 0.4971641914619625\n",
      "episode: 190   score: 52.0   memory length: 5000   epsilon: 0.4945360605377482\n",
      "episode: 191   score: 133.0   memory length: 5000   epsilon: 0.4879531521685949\n",
      "episode: 192   score: 56.0   memory length: 5000   epsilon: 0.48517959267528793\n",
      "episode: 193   score: 57.0   memory length: 5000   epsilon: 0.4823735561062996\n",
      "episode: 194   score: 85.0   memory length: 5000   epsilon: 0.4782427250134188\n",
      "episode: 195   score: 76.0   memory length: 5000   epsilon: 0.4745742144941194\n",
      "episode: 196   score: 85.0   memory length: 5000   epsilon: 0.47051017346969853\n",
      "episode: 197   score: 72.0   memory length: 5000   epsilon: 0.4670877849980194\n",
      "episode: 198   score: 44.0   memory length: 5000   epsilon: 0.4649905075135781\n",
      "episode: 199   score: 15.0   memory length: 5000   epsilon: 0.4642470804298554\n",
      "episode: 200   score: 22.0   memory length: 5000   epsilon: 0.4631804858682097\n",
      "episode: 201   score: 123.0   memory length: 5000   epsilon: 0.4574722267773922\n",
      "episode: 202   score: 70.0   memory length: 5000   epsilon: 0.45423551604967344\n",
      "episode: 203   score: 35.0   memory length: 5000   epsilon: 0.4526031266350782\n",
      "episode: 204   score: 80.0   memory length: 5000   epsilon: 0.44895166710972434\n",
      "episode: 205   score: 76.0   memory length: 5000   epsilon: 0.44550784281861167\n",
      "episode: 206   score: 51.0   memory length: 5000   epsilon: 0.4431970996362771\n",
      "episode: 207   score: 56.0   memory length: 5000   epsilon: 0.4406779366436019\n",
      "episode: 208   score: 55.0   memory length: 5000   epsilon: 0.4382169144391986\n",
      "episode: 209   score: 78.0   memory length: 5000   epsilon: 0.4347684676902515\n",
      "episode: 210   score: 78.0   memory length: 5000   epsilon: 0.43134715769615917\n",
      "episode: 211   score: 38.0   memory length: 5000   epsilon: 0.4296680961250465\n",
      "episode: 212   score: 92.0   memory length: 5000   epsilon: 0.4256905084011551\n",
      "episode: 213   score: 61.0   memory length: 5000   epsilon: 0.42305926098068647\n",
      "episode: 214   score: 73.0   memory length: 5000   epsilon: 0.4199400219042859\n",
      "episode: 215   score: 118.0   memory length: 5000   epsilon: 0.4149721049777049\n",
      "episode: 216   score: 75.0   memory length: 5000   epsilon: 0.41183011456549085\n",
      "episode: 217   score: 42.0   memory length: 5000   epsilon: 0.4100629588214767\n",
      "episode: 218   score: 49.0   memory length: 5000   epsilon: 0.4080176592708163\n",
      "episode: 219   score: 61.0   memory length: 5000   epsilon: 0.405495649988781\n",
      "episode: 220   score: 28.0   memory length: 5000   epsilon: 0.40432135743543407\n",
      "episode: 221   score: 120.0   memory length: 5000   epsilon: 0.3994583066472365\n",
      "episode: 222   score: 47.0   memory length: 5000   epsilon: 0.397545405763764\n",
      "episode: 223   score: 101.0   memory length: 5000   epsilon: 0.3935108520988879\n",
      "episode: 224   score: 37.0   memory length: 5000   epsilon: 0.39201827392544797\n",
      "episode: 225   score: 38.0   memory length: 5000   epsilon: 0.39049230393311585\n",
      "episode: 226   score: 125.0   memory length: 5000   epsilon: 0.38560272545714563\n",
      "episode: 227   score: 41.0   memory length: 5000   epsilon: 0.3839865096272853\n",
      "episode: 228   score: 92.0   memory length: 5000   epsilon: 0.3804318123141551\n",
      "episode: 229   score: 104.0   memory length: 5000   epsilon: 0.37645797872754994\n",
      "episode: 230   score: 33.0   memory length: 5000   epsilon: 0.3751801312781573\n",
      "episode: 231   score: 60.0   memory length: 5000   epsilon: 0.3728983847905867\n",
      "episode: 232   score: 109.0   memory length: 5000   epsilon: 0.3688187775519548\n",
      "episode: 233   score: 45.0   memory length: 5000   epsilon: 0.3671260228569082\n",
      "episode: 234   score: 53.0   memory length: 5000   epsilon: 0.3651487868122732\n",
      "episode: 235   score: 38.0   memory length: 5000   epsilon: 0.3637274089621224\n",
      "episode: 236   score: 90.0   memory length: 5000   epsilon: 0.3604323400875834\n",
      "episode: 237   score: 188.0   memory length: 5000   epsilon: 0.35368380597084054\n",
      "episode: 238   score: 85.0   memory length: 5000   epsilon: 0.3506550162615826\n",
      "episode: 239   score: 141.0   memory length: 5000   epsilon: 0.3457106558530396\n",
      "episode: 240   score: 90.0   memory length: 5000   epsilon: 0.342578803829712\n",
      "episode: 241   score: 89.0   memory length: 5000   epsilon: 0.3395092747175657\n",
      "episode: 242   score: 36.0   memory length: 5000   epsilon: 0.33825534889713416\n",
      "episode: 243   score: 176.0   memory length: 5000   epsilon: 0.3323206098683011\n",
      "episode: 244   score: 101.0   memory length: 5000   epsilon: 0.3289480005637562\n",
      "episode: 245   score: 64.0   memory length: 5000   epsilon: 0.326816666332299\n",
      "episode: 246   score: 114.0   memory length: 5000   epsilon: 0.32307961703472593\n",
      "episode: 247   score: 109.0   memory length: 5000   epsilon: 0.3195450403294675\n",
      "episode: 248   score: 119.0   memory length: 5000   epsilon: 0.3157332258822494\n",
      "episode: 249   score: 144.0   memory length: 5000   epsilon: 0.3111879000904685\n",
      "episode: 250   score: 62.0   memory length: 5000   epsilon: 0.3092334814805195\n",
      "episode: 251   score: 54.0   memory length: 5000   epsilon: 0.30753728134737224\n",
      "episode: 252   score: 85.0   memory length: 5000   epsilon: 0.304903669807253\n",
      "episode: 253   score: 247.0   memory length: 5000   epsilon: 0.2974346835951859\n",
      "episode: 254   score: 122.0   memory length: 5000   epsilon: 0.2937984637706827\n",
      "episode: 255   score: 196.0   memory length: 5000   epsilon: 0.28806698786230317\n",
      "episode: 256   score: 247.0   memory length: 5000   epsilon: 0.28101043665104497\n",
      "episode: 257   score: 99.0   memory length: 5000   epsilon: 0.27821419697174055\n",
      "episode: 258   score: 150.0   memory length: 5000   epsilon: 0.2740445144440535\n",
      "episode: 259   score: 161.0   memory length: 5000   epsilon: 0.26964054160700907\n",
      "episode: 260   score: 406.0   memory length: 5000   epsilon: 0.25888597388218954\n",
      "episode: 261   score: 236.0   memory length: 5000   epsilon: 0.2528222125212331\n",
      "episode: 262   score: 4954.0   memory length: 5000   epsilon: 0.15403221174966983\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gustavo\\Github\\RL-Projects\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    # Define size of state and actions\n",
    "    states = env.observation_space.shape[0]\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    # Define number of episodes\n",
    "    EPISODES = 300\n",
    "\n",
    "    # Define Agent\n",
    "    agent = DQNAgent(states, actions)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, states])\n",
    "\n",
    "\n",
    "        while not done:\n",
    "            if agent.render == 1:\n",
    "                env.render()\n",
    "\n",
    "            # Obtain action for the current state and go one step in environment  \n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, states])\n",
    "\n",
    "            # If an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100   \n",
    "\n",
    "            # Store the sample to memory\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            # Train the model\n",
    "            agent.training_loop()\n",
    "            # Update reward and state\n",
    "            score += reward\n",
    "            state = next_state\n",
    "    \n",
    "            if done:\n",
    "                # Update the target model with the network wheight \n",
    "                agent.update_target_model()\n",
    "                # Update the score calculation\n",
    "                score = score + 100 if not done else score\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                print(\"Episode:\", e, \"|  Score:\", score, \"|  Memory length:\",\n",
    "                        len(agent.memory), \"|  Epsilon:\", agent.epsilon)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m, episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for episode in range (1, episodes+1):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, states])\n",
    "\n",
    "    total_reward = 0\n",
    "    current_step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # Choose action using the trained model\n",
    "        action = np.argmax(agent.model.predict(state, verbose = 0)[0])\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, states])\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        print(f\"Test Episode: {episode}, Current Reward: {total_reward}, Cart Position: {state[0, 0]}, Pole Angle: {state[0, 2]}, Done: {done}\")\n",
    "\n",
    "    print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
